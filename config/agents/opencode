#!/bin/bash
# OpenCode wrapper script
# Copies AGENT.md to current directory, configures a model profile, then starts OpenCode

OPENCODE_CONFIG_DIR="${HOME}/.config/opencode"
OPENCODE_CONFIG_FILE="${OPENCODE_CONFIG_DIR}/opencode.json"
OPENCODE_DEFAULT_CONFIG_FILE="/opt/jovyan_defaults/.config/opencode/opencode.json"

# Copy AGENT.md from /opt to current directory
if [ ! -f ./AGENT.md ]; then
    cp /opt/AGENT.md ./AGENT.md
fi

ensure_opencode_config_file() {
    mkdir -p "${OPENCODE_CONFIG_DIR}"
    if [ ! -f "${OPENCODE_CONFIG_FILE}" ] && [ -f "${OPENCODE_DEFAULT_CONFIG_FILE}" ]; then
        cp "${OPENCODE_DEFAULT_CONFIG_FILE}" "${OPENCODE_CONFIG_FILE}"
    fi
}

# Switch OpenCode between Neurodesk, Jetstream, and local Ollama model profiles.
set_opencode_model_profile() {
    local profile="$1"

    ensure_opencode_config_file
    if [ ! -f "${OPENCODE_CONFIG_FILE}" ]; then
        return 0
    fi

    if ! command -v python3 >/dev/null 2>&1; then
        echo "Warning: python3 not found; cannot update OpenCode model profile." >&2
        return 1
    fi

    python3 - "${OPENCODE_CONFIG_FILE}" "${OPENCODE_DEFAULT_CONFIG_FILE}" "${profile}" <<'PY'
import json
import os
import sys
import copy

config_path, default_path, profile = sys.argv[1:4]

with open(config_path, "r", encoding="utf-8") as f:
    cfg = json.load(f)

default_cfg = {}
if os.path.exists(default_path):
    with open(default_path, "r", encoding="utf-8") as f:
        default_cfg = json.load(f)

providers = cfg.get("provider", {})
default_providers = default_cfg.get("provider", {})

ollama_host = os.environ.get("OLLAMA_HOST", "http://127.0.0.1:11434").rstrip("/")
if ollama_host.endswith("/v1"):
    ollama_host = ollama_host[:-3]

ollama_model_defaults = {
    "devstral-16k:latest": {
        "name": "devstral-16k:latest",
        "limit": {"context": 16384, "output": 8192},
    },
}

profile_defaults = {
    "neurodesk": {
        "provider_key": "neurodesk",
        "model": "neurodesk/devstral-small-2",
    },
    "jetstream": {
        "provider_key": "jetstream",
        "model": "jetstream/gpt-oss-120b",
    },
    "ollama": {
        "provider_key": "ollama",
        "model": "ollama/devstral-16k:latest",
        "provider_fallback": {
            "npm": "@ai-sdk/openai-compatible",
            "name": "Local Ollama",
            "options": {
                "baseURL": f"{ollama_host}/v1",
            },
            "models": copy.deepcopy(ollama_model_defaults),
        },
    },
}

if profile not in profile_defaults:
    raise SystemExit(f"Unknown profile: {profile}")

def ensure_provider(provider_key, provider_fallback=None):
    if provider_key in providers:
        return
    if provider_key in default_providers:
        providers[provider_key] = default_providers[provider_key]
    elif provider_fallback is not None:
        providers[provider_key] = provider_fallback

# Keep all known providers visible in OpenCode, even when one profile is selected.
ensure_provider("neurodesk")
ensure_provider("jetstream")
ensure_provider("ollama", profile_defaults["ollama"]["provider_fallback"])

profile_cfg = profile_defaults[profile]

# Keep Ollama target in sync when users override OLLAMA_HOST.
if isinstance(providers.get("ollama"), dict):
    options = providers["ollama"].setdefault("options", {})
    options["baseURL"] = f"{ollama_host}/v1"
    models = providers["ollama"].setdefault("models", {})
    for stale_model_name in tuple(models.keys()):
        if stale_model_name in {"neurodesk", "qwen3-coder-next:latest", "codellama:7b-code"}:
            models.pop(stale_model_name, None)
        elif stale_model_name.startswith("devstral-") and stale_model_name != "devstral-16k:latest":
            models.pop(stale_model_name, None)
        elif stale_model_name.startswith("devstral:"):
            models.pop(stale_model_name, None)
    for model_name, model_cfg in ollama_model_defaults.items():
        default_model_cfg = copy.deepcopy(model_cfg)
        current_model_cfg = models.get(model_name)
        if not isinstance(current_model_cfg, dict):
            models[model_name] = default_model_cfg
            continue

        current_model_cfg.setdefault("name", default_model_cfg.get("name", model_name))

        default_limit = default_model_cfg.get("limit")
        if isinstance(default_limit, dict):
            current_limit = current_model_cfg.get("limit")
            if not isinstance(current_limit, dict):
                current_model_cfg["limit"] = default_limit
            else:
                for limit_key, limit_value in default_limit.items():
                    current_limit.setdefault(limit_key, limit_value)

cfg["provider"] = providers
cfg["model"] = profile_cfg["model"]

tmp_path = f"{config_path}.tmp"
with open(tmp_path, "w", encoding="utf-8") as f:
    json.dump(cfg, f, indent=2)
    f.write("\n")
os.replace(tmp_path, config_path)
PY
}

# Activate/deactivate Jetstream gpt-oss-120b in OpenCode's provider model list.
set_jetstream_gpt_oss_model_state() {
    local action="$1"

    ensure_opencode_config_file
    if [ ! -f "${OPENCODE_CONFIG_FILE}" ]; then
        return 0
    fi

    if ! command -v python3 >/dev/null 2>&1; then
        echo "Warning: python3 not found; cannot update Jetstream gpt-oss-120b model state." >&2
        return 1
    fi

    python3 - "${OPENCODE_CONFIG_FILE}" "${OPENCODE_DEFAULT_CONFIG_FILE}" "${action}" <<'PY'
import copy
import json
import os
import sys

config_path, default_path, action = sys.argv[1:4]

with open(config_path, "r", encoding="utf-8") as f:
    cfg = json.load(f)

default_cfg = {}
if os.path.exists(default_path):
    with open(default_path, "r", encoding="utf-8") as f:
        default_cfg = json.load(f)

providers = cfg.setdefault("provider", {})
jetstream_provider = providers.setdefault("jetstream", {})
jetstream_models = jetstream_provider.setdefault("models", {})

default_model_cfg = (
    default_cfg.get("provider", {})
    .get("jetstream", {})
    .get("models", {})
    .get("gpt-oss-120b")
)

if action == "deactivate":
    jetstream_models.pop("gpt-oss-120b", None)
elif action == "activate":
    if "gpt-oss-120b" not in jetstream_models:
        if isinstance(default_model_cfg, dict):
            jetstream_models["gpt-oss-120b"] = copy.deepcopy(default_model_cfg)
        else:
            jetstream_models["gpt-oss-120b"] = {"name": "gpt-oss-120b"}
else:
    raise SystemExit(f"Unknown action: {action}")

tmp_path = f"{config_path}.tmp"
with open(tmp_path, "w", encoding="utf-8") as f:
    json.dump(cfg, f, indent=2)
    f.write("\n")
os.replace(tmp_path, config_path)
PY
}

# Return success when Jetstream's gpt-oss-120b endpoint explicitly denies permission.
jetstream_gpt_oss_permission_denied() {
    local response_file
    local http_code
    local endpoint="https://llm.jetstream-cloud.org/v1/chat/completions"

    if ! command -v curl >/dev/null 2>&1; then
        return 1
    fi

    response_file=$(mktemp)
    if [ -z "${response_file}" ]; then
        return 1
    fi

    http_code=$(curl -sS -m 12 -o "${response_file}" -w "%{http_code}" \
        -H "Content-Type: application/json" \
        -d '{"model":"gpt-oss-120b","messages":[{"role":"user","content":"ping"}],"max_tokens":1,"temperature":0}' \
        "${endpoint}" 2>/dev/null || true)

    if grep -Eiq 'permission[[:space:]_-]*denied|permission_denied' "${response_file}"; then
        rm -f "${response_file}"
        return 0
    fi

    if [ "${http_code}" = "401" ] || [ "${http_code}" = "403" ]; then
        if grep -Eiq 'forbidden|unauthorized|not allowed|access denied|permission' "${response_file}"; then
            rm -f "${response_file}"
            return 0
        fi
    fi

    rm -f "${response_file}"
    return 1
}

# Use Jetstream gpt-oss-120b when permitted, otherwise disable it and choose a non-Jetstream fallback.
set_jetstream_profile() {
    local fallback_choice

    if jetstream_gpt_oss_permission_denied; then
        echo "Jetstream gpt-oss-120b returned permission denied; deactivating it." >&2
        set_jetstream_gpt_oss_model_state deactivate

        if [ -t 0 ]; then
            echo "Choose fallback model:"
            echo "1) Local Ollama model"
            echo "2) Remote Neurodesk model"

            while true; do
                read -r -p "Choose [1/2]: " fallback_choice
                case "${fallback_choice}" in
                    1)
                        if ! is_local_ollama_available; then
                            echo "Local Ollama server not reachable at ${OLLAMA_HOST:-http://127.0.0.1:11434} right now." >&2
                            echo "Switching anyway; start Ollama before sending requests." >&2
                        fi
                        set_opencode_model_profile ollama
                        return 0
                        ;;
                    2)
                        if [ -z "${NEURODESK_API_KEY:-}" ]; then
                            echo "NEURODESK_API_KEY is not set. Remote Neurodesk requests may fail until it is configured." >&2
                        fi
                        set_opencode_model_profile neurodesk
                        return 0
                        ;;
                    *)
                        echo "Invalid choice. Enter 1 or 2."
                        ;;
                esac
            done
        fi

        if is_local_ollama_available; then
            echo "Local Ollama is reachable; switching OpenCode to Ollama model." >&2
            set_opencode_model_profile ollama
        else
            echo "Falling back to Neurodesk model (requires NEURODESK_API_KEY)." >&2
            set_opencode_model_profile neurodesk
        fi
        return 0
    fi

    set_jetstream_gpt_oss_model_state activate
    set_opencode_model_profile jetstream
}

# Check whether a local Ollama server appears reachable.
is_local_ollama_available() {
    local ollama_host="${OLLAMA_HOST:-http://127.0.0.1:11434}"
    ollama_host="${ollama_host%/}"
    ollama_host="${ollama_host%/v1}"

    if ! command -v curl >/dev/null 2>&1; then
        return 1
    fi

    curl -fsS "${ollama_host}/api/tags" >/dev/null 2>&1
}

# Ensure OpenCode has a usable model profile.
ensure_opencode_model_profile() {
    local bashrc_file="${HOME}/.bashrc"
    local key_from_bashrc
    local entered_key
    local tmp_bashrc
    local choice
    local requested_profile="${OPENCODE_MODEL_PROFILE:-}"

    if [ -n "${requested_profile}" ]; then
        case "${requested_profile}" in
            neurodesk|jetstream|ollama)
                if [ "${requested_profile}" = "ollama" ] && ! is_local_ollama_available; then
                    echo "OPENCODE_MODEL_PROFILE=ollama requested, but local Ollama is not reachable at ${OLLAMA_HOST:-http://127.0.0.1:11434} yet." >&2
                fi
                if [ "${requested_profile}" = "jetstream" ]; then
                    set_jetstream_profile
                else
                    set_opencode_model_profile "${requested_profile}"
                fi
                return 0
                ;;
            *)
                echo "Invalid OPENCODE_MODEL_PROFILE='${requested_profile}'. Expected: neurodesk, jetstream, or ollama." >&2
                ;;
        esac
    fi

    # Prefer local Ollama in explicit local-LLM sessions.
    if [ "${START_LOCAL_LLMS:-0}" = "1" ]; then
        if ! is_local_ollama_available; then
            echo "START_LOCAL_LLMS=1 is set, but local Ollama is not reachable at ${OLLAMA_HOST:-http://127.0.0.1:11434} yet." >&2
        fi
        set_opencode_model_profile ollama
        return 0
    fi

    # Use already-exported key when available.
    if [ -n "${NEURODESK_API_KEY:-}" ]; then
        set_opencode_model_profile neurodesk
        return 0
    fi

    # Try to recover key from an existing .bashrc export line first.
    if [ -f "${bashrc_file}" ]; then
        key_from_bashrc=$(sed -nE \
            -e "s/^[[:space:]]*export[[:space:]]+NEURODESK_API_KEY='([^']+)'[[:space:]]*$/\1/p" \
            -e 's/^[[:space:]]*export[[:space:]]+NEURODESK_API_KEY="([^"]+)"[[:space:]]*$/\1/p' \
            -e 's/^[[:space:]]*export[[:space:]]+NEURODESK_API_KEY=([^[:space:]#]+)[[:space:]]*$/\1/p' \
            "${bashrc_file}" | tail -n 1)
        if [ -n "${key_from_bashrc}" ]; then
            export NEURODESK_API_KEY="${key_from_bashrc}"
            set_opencode_model_profile neurodesk
            return 0
        fi
    fi

    if [ ! -t 0 ]; then
        if is_local_ollama_available; then
            echo "NEURODESK_API_KEY is not set. Local Ollama is available; using local model." >&2
            set_opencode_model_profile ollama
        else
            echo "NEURODESK_API_KEY is not set. Using Jetstream model for this session." >&2
            set_jetstream_profile
        fi
        return 0
    fi

    echo "NEURODESK_API_KEY is not set."
    echo "1) Create key at https://llm.neurodesk.org and use Neurodesk model"
    echo "2) Skip key creation and use Jetstream model"
    echo "3) Use local Ollama model (requires running Ollama server)"

    while true; do
        read -r -p "Choose [1/2/3]: " choice
        case "${choice}" in
            1)
                echo "Go to https://llm.neurodesk.org, create an API key, then paste it below."
                while [ -z "${entered_key:-}" ]; do
                    read -r -s -p "Paste your API key: " entered_key
                    echo
                    if [ -z "${entered_key}" ]; then
                        echo "API key cannot be empty. Please try again."
                    fi
                done

                # Persist the key to ~/.bashrc and export it for this process.
                touch "${bashrc_file}"
                tmp_bashrc=$(mktemp)
                grep -v '^[[:space:]]*export[[:space:]]\+NEURODESK_API_KEY=' "${bashrc_file}" > "${tmp_bashrc}" || true
                echo "" >> "${tmp_bashrc}"
                echo "# Neurodesk API key for OpenCode" >> "${tmp_bashrc}"
                printf "export NEURODESK_API_KEY='%s'\n" "${entered_key}" >> "${tmp_bashrc}"
                mv "${tmp_bashrc}" "${bashrc_file}"

                export NEURODESK_API_KEY="${entered_key}"
                set_opencode_model_profile neurodesk
                echo "Saved NEURODESK_API_KEY to ${bashrc_file}."
                return 0
                ;;
            2)
                echo "Skipping key creation. Switching OpenCode to Jetstream model."
                set_jetstream_profile
                return 0
                ;;
            3)
                if ! is_local_ollama_available; then
                    echo "Local Ollama server not reachable at ${OLLAMA_HOST:-http://127.0.0.1:11434} right now."
                    echo "Switching anyway; start Ollama before sending requests."
                fi
                echo "Switching OpenCode to local Ollama model."
                set_opencode_model_profile ollama
                return 0
                ;;
            *)
                echo "Invalid choice. Enter 1, 2, or 3."
                ;;
        esac
    done
}

ensure_opencode_model_profile

# Start OpenCode using the actual binary path (not this wrapper)
export TERM=xterm-direct
exec /usr/bin/opencode "$@"
