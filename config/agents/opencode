#!/bin/bash
# OpenCode wrapper script
# Copies AGENTS.md, discovers working model backends, updates OpenCode config,
# lets the user choose a default model, then starts OpenCode.

OPENCODE_CONFIG_DIR="${HOME}/.config/opencode"
OPENCODE_CONFIG_FILE="${OPENCODE_CONFIG_DIR}/opencode.json"
OPENCODE_DEFAULT_CONFIG_FILE="/opt/jovyan_defaults/.config/opencode/opencode.json"

JETSTREAM_BASE_URL="https://llm.jetstream-cloud.org/v1"
JETSTREAM_MODEL_ID="gpt-oss-120b"
NEURODESK_BASE_URL="https://llm.neurodesk.org/v1"
NEURODESK_DEFAULT_MODEL_ID="devstral-small-2"

OLLAMA_HOST_CLEAN="${OLLAMA_HOST:-http://127.0.0.1:11434}"
OLLAMA_HOST_CLEAN="${OLLAMA_HOST_CLEAN%/}"
OLLAMA_HOST_CLEAN="${OLLAMA_HOST_CLEAN%/v1}"

JETSTREAM_STATUS="not checked"
OLLAMA_RUNNING=0
OLLAMA_STATUS="not checked"
NEURODESK_RUNNING=0
NEURODESK_REQUIRES_KEY=0
NEURODESK_STATUS="not checked"
OLLAMA_MODEL_CONTEXTS_JSON="{}"
OLLAMA_DEFAULT_CONTEXT_LIMIT=32768
OLLAMA_DEFAULT_OUTPUT_LIMIT=8192

JETSTREAM_MODELS=()
OLLAMA_MODELS=()
NEURODESK_MODELS=()

WORKING_MODELS=()
WORKING_MODEL_LABELS=()
SELECTED_MODEL=""

# Copy AGENTS.md from /opt to current directory.
if [ ! -f ./AGENTS.md ]; then
    cp /opt/AGENTS.md ./AGENTS.md
fi

ensure_opencode_config_file() {
    mkdir -p "${OPENCODE_CONFIG_DIR}"
    if [ ! -f "${OPENCODE_CONFIG_FILE}" ] && [ -f "${OPENCODE_DEFAULT_CONFIG_FILE}" ]; then
        cp "${OPENCODE_DEFAULT_CONFIG_FILE}" "${OPENCODE_CONFIG_FILE}"
    fi
}

array_contains() {
    local needle="$1"
    shift
    local value
    for value in "$@"; do
        if [ "${value}" = "${needle}" ]; then
            return 0
        fi
    done
    return 1
}

json_array_from_args() {
    python3 - "$@" <<'PY'
import json
import sys

print(json.dumps(sys.argv[1:]))
PY
}

sanitize_neurodesk_api_key() {
    local raw_key="$1"
    local sanitized_key

    # Remove control characters that can break headers/config parsing.
    sanitized_key=$(printf "%s" "${raw_key}" | tr -d '\000-\010\013\014\016-\037\177')
    # Normalize CRLF/newline artifacts and trim surrounding whitespace.
    sanitized_key="${sanitized_key//$'\r'/}"
    sanitized_key="${sanitized_key//$'\n'/}"
    sanitized_key=$(printf "%s" "${sanitized_key}" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')

    printf "%s" "${sanitized_key}"
}

load_neurodesk_api_key_from_bashrc() {
    local bashrc_file="${HOME}/.bashrc"
    local key_from_bashrc

    if [ -n "${NEURODESK_API_KEY:-}" ]; then
        NEURODESK_API_KEY=$(sanitize_neurodesk_api_key "${NEURODESK_API_KEY}")
        export NEURODESK_API_KEY
        return 0
    fi

    if [ ! -f "${bashrc_file}" ]; then
        return 0
    fi

    key_from_bashrc=$(sed -nE \
        -e "s/^[[:space:]]*export[[:space:]]+NEURODESK_API_KEY='([^']+)'[[:space:]]*$/\1/p" \
        -e 's/^[[:space:]]*export[[:space:]]+NEURODESK_API_KEY="([^"]+)"[[:space:]]*$/\1/p' \
        -e 's/^[[:space:]]*export[[:space:]]+NEURODESK_API_KEY=([^[:space:]#]+)[[:space:]]*$/\1/p' \
        "${bashrc_file}" | tail -n 1)

    key_from_bashrc=$(sanitize_neurodesk_api_key "${key_from_bashrc}")

    if [ -n "${key_from_bashrc}" ]; then
        export NEURODESK_API_KEY="${key_from_bashrc}"
    fi
}

persist_neurodesk_api_key_to_bashrc() {
    local key="$1"
    local escaped_key
    local bashrc_file="${HOME}/.bashrc"
    local tmp_bashrc

    escaped_key=$(printf "%s" "${key}" | sed "s/'/'\"'\"'/g")

    touch "${bashrc_file}"
    tmp_bashrc=$(mktemp) || return 1

    grep -v '^[[:space:]]*export[[:space:]]\+NEURODESK_API_KEY=' "${bashrc_file}" > "${tmp_bashrc}" || true
    {
        echo ""
        echo "# Neurodesk API key for OpenCode"
        printf "export NEURODESK_API_KEY='%s'\n" "${escaped_key}"
    } >> "${tmp_bashrc}"

    mv "${tmp_bashrc}" "${bashrc_file}"
}

# Step 1: Check if Jetstream's gpt-oss-120b model API call works.
check_jetstream_model_api() {
    local endpoint="${JETSTREAM_BASE_URL}/chat/completions"
    local response_file
    local http_code

    JETSTREAM_MODELS=()

    if ! command -v curl >/dev/null 2>&1; then
        JETSTREAM_STATUS="curl not found"
        return 1
    fi

    response_file=$(mktemp) || {
        JETSTREAM_STATUS="failed to create temporary file"
        return 1
    }

    http_code=$(curl -sS -m 12 -o "${response_file}" -w "%{http_code}" \
        -H "Content-Type: application/json" \
        -d '{"model":"gpt-oss-120b","messages":[{"role":"user","content":"ping"}],"max_tokens":1,"temperature":0}' \
        "${endpoint}" 2>/dev/null || true)

    if [ "${http_code}" = "200" ]; then
        JETSTREAM_MODELS=("${JETSTREAM_MODEL_ID}")
        JETSTREAM_STATUS="working (${JETSTREAM_MODEL_ID})"
        rm -f "${response_file}"
        return 0
    fi

    if grep -Eiq 'permission[[:space:]_-]*denied|permission_denied' "${response_file}"; then
        JETSTREAM_STATUS="reachable but permission denied for ${JETSTREAM_MODEL_ID}"
    elif [ -n "${http_code}" ] && [ "${http_code}" != "000" ]; then
        JETSTREAM_STATUS="unavailable (HTTP ${http_code})"
    else
        JETSTREAM_STATUS="unreachable"
    fi

    rm -f "${response_file}"
    return 1
}

# Step 2: Check local Ollama and list available local models.
check_local_ollama_models() {
    local response_file
    local http_code
    local model_name

    OLLAMA_RUNNING=0
    OLLAMA_MODELS=()
    OLLAMA_MODEL_CONTEXTS_JSON="{}"

    if ! command -v curl >/dev/null 2>&1; then
        OLLAMA_STATUS="curl not found"
        return 1
    fi

    response_file=$(mktemp) || {
        OLLAMA_STATUS="failed to create temporary file"
        return 1
    }

    http_code=$(curl -sS -m 8 -o "${response_file}" -w "%{http_code}" \
        "${OLLAMA_HOST_CLEAN}/api/tags" 2>/dev/null || true)

    if [ "${http_code}" = "200" ]; then
        OLLAMA_RUNNING=1

        if command -v python3 >/dev/null 2>&1; then
            while IFS= read -r model_name; do
                if [ -n "${model_name}" ]; then
                    OLLAMA_MODELS+=("${model_name}")
                fi
            done < <(python3 - "${response_file}" <<'PY'
import json
import sys

try:
    with open(sys.argv[1], "r", encoding="utf-8") as f:
        payload = json.load(f)
except Exception:
    raise SystemExit(0)

for model in payload.get("models", []):
    name = model.get("name")
    if isinstance(name, str) and name:
        print(name)
PY
            )

            if [ "${#OLLAMA_MODELS[@]}" -gt 0 ]; then
                OLLAMA_MODEL_CONTEXTS_JSON=$(python3 - "${OLLAMA_HOST_CLEAN}" "${OLLAMA_DEFAULT_CONTEXT_LIMIT}" "${OLLAMA_MODELS[@]}" <<'PY'
import json
import re
import sys
from urllib import error, request

base_url = sys.argv[1].rstrip("/")
fallback_context = int(sys.argv[2])
model_ids = sys.argv[3:]

def positive_int(value):
    try:
        parsed = int(value)
    except (TypeError, ValueError):
        return None
    return parsed if parsed > 0 else None

def extract_context(payload):
    if not isinstance(payload, dict):
        return None

    candidates = []

    for field in ("context_length", "num_ctx", "max_context_length", "max_sequence_length"):
        candidates.append(payload.get(field))

    details = payload.get("details")
    if isinstance(details, dict):
        for field in ("context_length", "num_ctx", "max_context_length", "max_sequence_length"):
            candidates.append(details.get(field))

    model_info = payload.get("model_info")
    if isinstance(model_info, dict):
        for key, value in model_info.items():
            if key in ("context_length", "num_ctx", "max_context_length", "max_sequence_length"):
                candidates.append(value)
                continue
            if isinstance(key, str) and key.endswith(".context_length"):
                candidates.append(value)

    for raw in (payload.get("parameters"), payload.get("modelfile")):
        if not isinstance(raw, str):
            continue
        match = re.search(r"(?im)^\\s*(?:parameter\\s+)?(?:num_ctx|context_length)\\s+([0-9]+)\\s*$", raw)
        if match:
            candidates.append(match.group(1))

    for value in candidates:
        parsed = positive_int(value)
        if parsed is not None:
            return parsed
    return None

def fetch_show_payload(model_id):
    endpoint = f"{base_url}/api/show"
    body = json.dumps({"name": model_id}).encode("utf-8")
    req = request.Request(endpoint, data=body, method="POST")
    req.add_header("Content-Type", "application/json")
    with request.urlopen(req, timeout=6) as resp:
        if resp.status != 200:
            return None
        data = resp.read()
    return json.loads(data.decode("utf-8"))

contexts = {}
for model_id in model_ids:
    if not isinstance(model_id, str) or not model_id:
        continue
    context_limit = None
    try:
        show_payload = fetch_show_payload(model_id)
        context_limit = extract_context(show_payload)
    except (error.URLError, TimeoutError, ValueError, json.JSONDecodeError):
        context_limit = None

    parsed_context = positive_int(context_limit)
    if parsed_context is None:
        parsed_context = fallback_context

    contexts[model_id] = parsed_context

print(json.dumps(contexts))
PY
                )
            fi
        fi

        if [ "${#OLLAMA_MODELS[@]}" -gt 0 ]; then
            OLLAMA_STATUS="running with ${#OLLAMA_MODELS[@]} model(s): ${OLLAMA_MODELS[*]}"
        else
            OLLAMA_STATUS="running but no local models are available"
        fi

        rm -f "${response_file}"
        return 0
    fi

    if [ -n "${http_code}" ] && [ "${http_code}" != "000" ]; then
        OLLAMA_STATUS="not running or unreachable at ${OLLAMA_HOST_CLEAN} (HTTP ${http_code})"
    else
        OLLAMA_STATUS="not running or unreachable at ${OLLAMA_HOST_CLEAN}"
    fi

    rm -f "${response_file}"
    return 1
}

# Step 3: Check llm.neurodesk.org endpoint status.
check_neurodesk_server() {
    local endpoint="${NEURODESK_BASE_URL}/models"
    local response_file
    local http_code
    local model_name

    NEURODESK_RUNNING=0
    NEURODESK_REQUIRES_KEY=0
    NEURODESK_MODELS=()

    if ! command -v curl >/dev/null 2>&1; then
        NEURODESK_STATUS="curl not found"
        return 1
    fi

    response_file=$(mktemp) || {
        NEURODESK_STATUS="failed to create temporary file"
        return 1
    }

    if [ -n "${NEURODESK_API_KEY:-}" ]; then
        http_code=$(curl -sS -m 10 -o "${response_file}" -w "%{http_code}" \
            -H "Authorization: Bearer ${NEURODESK_API_KEY}" \
            "${endpoint}" 2>/dev/null || true)
    else
        http_code=$(curl -sS -m 10 -o "${response_file}" -w "%{http_code}" \
            "${endpoint}" 2>/dev/null || true)
    fi

    if [ "${http_code}" = "200" ]; then
        NEURODESK_RUNNING=1
        if command -v python3 >/dev/null 2>&1; then
            while IFS= read -r model_name; do
                if [ -n "${model_name}" ]; then
                    NEURODESK_MODELS+=("${model_name}")
                fi
            done < <(python3 - "${response_file}" <<'PY'
import json
import sys

try:
    with open(sys.argv[1], "r", encoding="utf-8") as f:
        payload = json.load(f)
except Exception:
    raise SystemExit(0)

for model in payload.get("data", []):
    model_id = model.get("id")
    if isinstance(model_id, str) and model_id:
        print(model_id)
PY
            )
        fi

        if [ "${#NEURODESK_MODELS[@]}" -eq 0 ]; then
            NEURODESK_MODELS=("${NEURODESK_DEFAULT_MODEL_ID}")
        fi

        NEURODESK_STATUS="running"
        rm -f "${response_file}"
        return 0
    fi

    if [ "${http_code}" = "401" ] || [ "${http_code}" = "403" ]; then
        NEURODESK_RUNNING=1
        NEURODESK_REQUIRES_KEY=1
        NEURODESK_MODELS=("${NEURODESK_DEFAULT_MODEL_ID}")

        if [ -n "${NEURODESK_API_KEY:-}" ]; then
            NEURODESK_STATUS="running, but the current NEURODESK_API_KEY is rejected (HTTP ${http_code})"
        else
            NEURODESK_STATUS="running, API key required (HTTP ${http_code})"
        fi

        rm -f "${response_file}"
        return 0
    fi

    if [ -n "${http_code}" ] && [ "${http_code}" != "000" ]; then
        NEURODESK_STATUS="unavailable (HTTP ${http_code})"
    else
        NEURODESK_STATUS="unreachable"
    fi

    rm -f "${response_file}"
    return 1
}

build_working_model_choices() {
    local model_name
    local neurodesk_suffix=""

    WORKING_MODELS=()
    WORKING_MODEL_LABELS=()

    for model_name in "${JETSTREAM_MODELS[@]}"; do
        WORKING_MODELS+=("jetstream/${model_name}")
        WORKING_MODEL_LABELS+=("Jetstream / ${model_name}")
    done

    if [ "${OLLAMA_RUNNING}" -eq 1 ]; then
        for model_name in "${OLLAMA_MODELS[@]}"; do
            WORKING_MODELS+=("ollama/${model_name}")
            WORKING_MODEL_LABELS+=("Local Ollama / ${model_name}")
        done
    fi

    if [ "${NEURODESK_RUNNING}" -eq 1 ] && [ "${#NEURODESK_MODELS[@]}" -gt 0 ]; then
        if [ "${NEURODESK_REQUIRES_KEY}" -eq 1 ] && [ -z "${NEURODESK_API_KEY:-}" ]; then
            neurodesk_suffix=" (requires API key setup)"
        fi

        for model_name in "${NEURODESK_MODELS[@]}"; do
            WORKING_MODELS+=("neurodesk/${model_name}")
            WORKING_MODEL_LABELS+=("llm.neurodesk.org / ${model_name}${neurodesk_suffix}")
        done
    fi
}

# Step 4 and Step 6: Update possible models in config and optionally set default model.
update_opencode_config() {
    local selected_model="${1:-}"
    local jetstream_models_json
    local ollama_models_json
    local ollama_model_contexts_json
    local neurodesk_models_json

    ensure_opencode_config_file
    if [ ! -f "${OPENCODE_CONFIG_FILE}" ]; then
        return 0
    fi

    if ! command -v python3 >/dev/null 2>&1; then
        echo "Warning: python3 not found; cannot update OpenCode config." >&2
        return 1
    fi

    jetstream_models_json=$(json_array_from_args "${JETSTREAM_MODELS[@]}")
    ollama_models_json=$(json_array_from_args "${OLLAMA_MODELS[@]}")
    ollama_model_contexts_json="${OLLAMA_MODEL_CONTEXTS_JSON:-{}}"
    neurodesk_models_json=$(json_array_from_args "${NEURODESK_MODELS[@]}")

    JETSTREAM_MODELS_JSON="${jetstream_models_json}" \
    OLLAMA_MODELS_JSON="${ollama_models_json}" \
    OLLAMA_MODEL_CONTEXTS_JSON="${ollama_model_contexts_json}" \
    OLLAMA_DEFAULT_CONTEXT_LIMIT="${OLLAMA_DEFAULT_CONTEXT_LIMIT}" \
    OLLAMA_DEFAULT_OUTPUT_LIMIT="${OLLAMA_DEFAULT_OUTPUT_LIMIT}" \
    NEURODESK_MODELS_JSON="${neurodesk_models_json}" \
    OLLAMA_BASE_URL="${OLLAMA_HOST_CLEAN}/v1" \
    JETSTREAM_BASE_URL="${JETSTREAM_BASE_URL}" \
    NEURODESK_BASE_URL="${NEURODESK_BASE_URL}" \
    python3 - "${OPENCODE_CONFIG_FILE}" "${OPENCODE_DEFAULT_CONFIG_FILE}" "${selected_model}" <<'PY'
import copy
import json
import os
import sys

config_path, default_path, selected_model = sys.argv[1:4]

with open(config_path, "r", encoding="utf-8") as f:
    cfg = json.load(f)

default_cfg = {}
if os.path.exists(default_path):
    with open(default_path, "r", encoding="utf-8") as f:
        default_cfg = json.load(f)

providers = cfg.get("provider")
if not isinstance(providers, dict):
    providers = {}

default_providers = default_cfg.get("provider")
if not isinstance(default_providers, dict):
    default_providers = {}

def load_json_env(name, default):
    raw = os.environ.get(name)
    if raw is None or raw == "":
        return copy.deepcopy(default)
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        # Recover from accidental concatenated payloads (e.g. "<json><json>" or trailing junk).
        try:
            parsed, _ = json.JSONDecoder().raw_decode(raw.lstrip())
            return parsed
        except json.JSONDecodeError:
            return copy.deepcopy(default)

jetstream_models = load_json_env("JETSTREAM_MODELS_JSON", [])
ollama_models = load_json_env("OLLAMA_MODELS_JSON", [])
ollama_model_contexts = load_json_env("OLLAMA_MODEL_CONTEXTS_JSON", {})
neurodesk_models = load_json_env("NEURODESK_MODELS_JSON", [])
ollama_base_url = os.environ.get("OLLAMA_BASE_URL", "http://127.0.0.1:11434/v1")
jetstream_base_url = os.environ.get("JETSTREAM_BASE_URL", "https://llm.jetstream-cloud.org/v1")
neurodesk_base_url = os.environ.get("NEURODESK_BASE_URL", "https://llm.neurodesk.org/v1")
ollama_default_context_limit = os.environ.get("OLLAMA_DEFAULT_CONTEXT_LIMIT", "16384")
ollama_default_output_limit = os.environ.get("OLLAMA_DEFAULT_OUTPUT_LIMIT", "8192")

if not isinstance(ollama_model_contexts, dict):
    ollama_model_contexts = {}

def positive_int(value):
    try:
        parsed = int(value)
    except (TypeError, ValueError):
        return None
    return parsed if parsed > 0 else None

OLLAMA_DEFAULT_CONTEXT_LIMIT = positive_int(ollama_default_context_limit) or 16384
OLLAMA_DEFAULT_OUTPUT_LIMIT = positive_int(ollama_default_output_limit) or 8192

fallbacks = {
    "neurodesk": {
        "npm": "@ai-sdk/openai-compatible",
        "name": "Neurodesk vLLM",
        "options": {
            "baseURL": neurodesk_base_url,
            "apiKey": "{env:NEURODESK_API_KEY}",
        },
        "models": {},
    },
    "jetstream": {
        "npm": "@ai-sdk/openai-compatible",
        "name": "JetStream",
        "options": {
            "baseURL": jetstream_base_url,
        },
        "models": {},
    },
    "ollama": {
        "npm": "@ai-sdk/openai-compatible",
        "name": "Local Ollama",
        "options": {
            "baseURL": ollama_base_url,
        },
        "models": {},
    },
}

def ensure_provider(provider_key: str):
    existing = providers.get(provider_key)
    if isinstance(existing, dict):
        return existing

    default_provider = default_providers.get(provider_key)
    if isinstance(default_provider, dict):
        providers[provider_key] = copy.deepcopy(default_provider)
        return providers[provider_key]

    providers[provider_key] = copy.deepcopy(fallbacks[provider_key])
    return providers[provider_key]

def apply_models(provider_key: str, model_ids):
    provider_cfg = ensure_provider(provider_key)
    options = provider_cfg.get("options")
    if not isinstance(options, dict):
        options = {}
        provider_cfg["options"] = options

    if provider_key == "ollama":
        options["baseURL"] = ollama_base_url
    elif provider_key == "jetstream":
        options["baseURL"] = jetstream_base_url
    elif provider_key == "neurodesk":
        options["baseURL"] = neurodesk_base_url
        options.setdefault("apiKey", "{env:NEURODESK_API_KEY}")

    existing_models = provider_cfg.get("models")
    if not isinstance(existing_models, dict):
        existing_models = {}

    default_models = (
        default_providers.get(provider_key, {}).get("models", {})
        if isinstance(default_providers.get(provider_key), dict)
        else {}
    )
    if not isinstance(default_models, dict):
        default_models = {}

    updated_models = {}
    for model_id in model_ids:
        if not isinstance(model_id, str) or not model_id:
            continue

        model_cfg = existing_models.get(model_id)
        if isinstance(model_cfg, dict):
            model_cfg = copy.deepcopy(model_cfg)
        else:
            default_model_cfg = default_models.get(model_id)
            if isinstance(default_model_cfg, dict):
                model_cfg = copy.deepcopy(default_model_cfg)
            else:
                model_cfg = {"name": model_id}

        model_cfg.setdefault("name", model_id)

        if provider_key == "ollama":
            limit_cfg = model_cfg.get("limit")
            if not isinstance(limit_cfg, dict):
                limit_cfg = {}

            context_limit = positive_int(limit_cfg.get("context"))
            if context_limit is None:
                context_limit = positive_int(ollama_model_contexts.get(model_id))
            if context_limit is None:
                context_limit = OLLAMA_DEFAULT_CONTEXT_LIMIT

            output_limit = positive_int(limit_cfg.get("output"))
            if output_limit is None:
                output_limit = OLLAMA_DEFAULT_OUTPUT_LIMIT
            if output_limit > context_limit:
                output_limit = context_limit

            limit_cfg["context"] = context_limit
            limit_cfg["output"] = output_limit
            model_cfg["limit"] = limit_cfg

        updated_models[model_id] = model_cfg

    provider_cfg["models"] = updated_models

apply_models("jetstream", jetstream_models)
apply_models("ollama", ollama_models)
apply_models("neurodesk", neurodesk_models)

cfg["provider"] = providers

def model_exists(model_ref):
    if not isinstance(model_ref, str) or "/" not in model_ref:
        return False

    provider_key, model_id = model_ref.split("/", 1)
    provider_cfg = providers.get(provider_key)
    if not isinstance(provider_cfg, dict):
        return False

    models_cfg = provider_cfg.get("models")
    if not isinstance(models_cfg, dict):
        return False

    return model_id in models_cfg

if selected_model and model_exists(selected_model):
    cfg["model"] = selected_model
else:
    current_model = cfg.get("model")
    if not model_exists(current_model):
        for provider_key in ("jetstream", "ollama", "neurodesk"):
            models_cfg = providers.get(provider_key, {}).get("models", {})
            if isinstance(models_cfg, dict) and models_cfg:
                first_model_id = next(iter(models_cfg.keys()))
                cfg["model"] = f"{provider_key}/{first_model_id}"
                break

tmp_path = f"{config_path}.tmp"
with open(tmp_path, "w", encoding="utf-8") as f:
    json.dump(cfg, f, indent=2)
    f.write("\n")
os.replace(tmp_path, config_path)
PY
}

resolve_requested_model_override() {
    local requested="${OPENCODE_MODEL_PROFILE:-}"
    local model_ref

    if [ -z "${requested}" ]; then
        return 1
    fi

    case "${requested}" in
        neurodesk|jetstream|ollama)
            for model_ref in "${WORKING_MODELS[@]}"; do
                case "${model_ref}" in
                    "${requested}/"*)
                        SELECTED_MODEL="${model_ref}"
                        echo "OPENCODE_MODEL_PROFILE=${requested} requested; using ${SELECTED_MODEL}."
                        return 0
                        ;;
                esac
            done
            echo "OPENCODE_MODEL_PROFILE=${requested} requested, but no working model for ${requested} is available." >&2
            return 1
            ;;
        */*)
            if array_contains "${requested}" "${WORKING_MODELS[@]}"; then
                SELECTED_MODEL="${requested}"
                echo "OPENCODE_MODEL_PROFILE=${requested} requested; using it."
                return 0
            fi
            echo "OPENCODE_MODEL_PROFILE=${requested} requested, but this model is not currently working." >&2
            return 1
            ;;
        *)
            echo "Invalid OPENCODE_MODEL_PROFILE='${requested}'. Use neurodesk, jetstream, ollama, or provider/model." >&2
            return 1
            ;;
    esac
}

# Step 5: Ask the user which working model should become the default.
choose_default_model() {
    local idx
    local choice

    SELECTED_MODEL=""

    if resolve_requested_model_override; then
        return 0
    fi

    if [ "${#WORKING_MODELS[@]}" -eq 0 ]; then
        return 1
    fi

    if [ "${#WORKING_MODELS[@]}" -eq 1 ] || [ ! -t 0 ]; then
        SELECTED_MODEL="${WORKING_MODELS[0]}"
        return 0
    fi

    echo "Working models detected:"
    for idx in "${!WORKING_MODELS[@]}"; do
        printf "%d) %s\n" "$((idx + 1))" "${WORKING_MODEL_LABELS[$idx]}"
    done

    while true; do
        read -r -p "Choose the default model [1-${#WORKING_MODELS[@]}]: " choice
        case "${choice}" in
            ''|*[!0-9]*)
                echo "Invalid choice. Enter a number."
                ;;
            *)
                if [ "${choice}" -ge 1 ] && [ "${choice}" -le "${#WORKING_MODELS[@]}" ]; then
                    SELECTED_MODEL="${WORKING_MODELS[$((choice - 1))]}"
                    return 0
                fi
                echo "Invalid choice. Enter a number between 1 and ${#WORKING_MODELS[@]}."
                ;;
        esac
    done
}

# Step 7: If user chose llm.neurodesk.org, guide them through API key setup/export.
ensure_neurodesk_api_key_for_selection() {
    local entered_key=""

    case "${SELECTED_MODEL}" in
        neurodesk/*)
            ;;
        *)
            return 0
            ;;
    esac

    load_neurodesk_api_key_from_bashrc
    if [ -n "${NEURODESK_API_KEY:-}" ] && [ "${NEURODESK_REQUIRES_KEY}" -eq 0 ]; then
        return 0
    fi

    if [ ! -t 0 ]; then
        echo "Selected ${SELECTED_MODEL}, but NEURODESK_API_KEY is not set and no interactive prompt is possible." >&2
        return 1
    fi

    if [ -n "${NEURODESK_API_KEY:-}" ] && [ "${NEURODESK_REQUIRES_KEY}" -eq 1 ]; then
        echo "The existing NEURODESK_API_KEY was rejected by llm.neurodesk.org."
        echo "Please enter a new key."
    fi

    echo "You selected llm.neurodesk.org."
    echo "1) Open https://llm.neurodesk.org and create an account (if needed)."
    echo "2) Create an API key in the Neurodesk web app."
    echo "3) Paste the key below; this wrapper will export it and save it to ~/.bashrc."

    while [ -z "${entered_key}" ]; do
        read -r -s -p "Paste Neurodesk API key: " entered_key
        echo
        if [ -z "${entered_key}" ]; then
            echo "API key cannot be empty. Please try again."
        fi
    done

    export NEURODESK_API_KEY="${entered_key}"
    if persist_neurodesk_api_key_to_bashrc "${entered_key}"; then
        echo "Saved NEURODESK_API_KEY to ${HOME}/.bashrc and exported it for this session."
    else
        echo "Warning: failed to save key to ${HOME}/.bashrc, but it is exported for this session." >&2
    fi

    return 0
}

ensure_opencode_model_selection() {
    local selected_neurodesk_model=""

    ensure_opencode_config_file
    if [ ! -f "${OPENCODE_CONFIG_FILE}" ]; then
        echo "Warning: OpenCode config not found at ${OPENCODE_CONFIG_FILE}; skipping model setup." >&2
        return 0
    fi

    if ! command -v python3 >/dev/null 2>&1; then
        echo "Warning: python3 not found; skipping OpenCode model setup." >&2
        return 0
    fi

    load_neurodesk_api_key_from_bashrc

    echo "Checking Jetstream model API (${JETSTREAM_MODEL_ID})..."
    check_jetstream_model_api

    echo "Checking local Ollama at ${OLLAMA_HOST_CLEAN}..."
    check_local_ollama_models

    echo "Checking llm.neurodesk.org..."
    check_neurodesk_server

    echo "Jetstream: ${JETSTREAM_STATUS}"
    echo "Ollama: ${OLLAMA_STATUS}"
    echo "llm.neurodesk.org: ${NEURODESK_STATUS}"

    build_working_model_choices

    # Step 4: Always refresh OpenCode providers/models using detected possibilities.
    update_opencode_config ""

    if [ "${#WORKING_MODELS[@]}" -eq 0 ]; then
        echo "No working models detected. Keeping the current OpenCode default model." >&2
        return 0
    fi

    if ! choose_default_model; then
        echo "Could not determine a model selection. Keeping the current OpenCode default model." >&2
        return 0
    fi

    if ! ensure_neurodesk_api_key_for_selection; then
        echo "Neurodesk key setup was not completed. Keeping the current OpenCode default model." >&2
        return 0
    fi

    if [[ "${SELECTED_MODEL}" == neurodesk/* ]]; then
        selected_neurodesk_model="${SELECTED_MODEL#neurodesk/}"
        check_neurodesk_server
        if ! array_contains "${selected_neurodesk_model}" "${NEURODESK_MODELS[@]}"; then
            NEURODESK_MODELS=("${selected_neurodesk_model}" "${NEURODESK_MODELS[@]}")
        fi
    fi

    # Step 6: Set selected model as default.
    update_opencode_config "${SELECTED_MODEL}"
    echo "OpenCode default model set to ${SELECTED_MODEL}."
}

ensure_opencode_model_selection

# Start OpenCode using the actual binary path (not this wrapper).
export TERM=xterm-direct
exec /usr/bin/opencode "$@"
