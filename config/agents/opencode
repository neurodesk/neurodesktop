#!/bin/bash
# OpenCode wrapper script
# Copies AGENT.md to current directory, configures a model profile, then starts OpenCode

OPENCODE_CONFIG_DIR="${HOME}/.config/opencode"
OPENCODE_CONFIG_FILE="${OPENCODE_CONFIG_DIR}/opencode.json"
OPENCODE_DEFAULT_CONFIG_FILE="/opt/jovyan_defaults/.config/opencode/opencode.json"

# Copy AGENT.md from /opt to current directory
if [ ! -f ./AGENT.md ]; then
    cp /opt/AGENT.md ./AGENT.md
fi

ensure_opencode_config_file() {
    mkdir -p "${OPENCODE_CONFIG_DIR}"
    if [ ! -f "${OPENCODE_CONFIG_FILE}" ] && [ -f "${OPENCODE_DEFAULT_CONFIG_FILE}" ]; then
        cp "${OPENCODE_DEFAULT_CONFIG_FILE}" "${OPENCODE_CONFIG_FILE}"
    fi
}

# Switch OpenCode between Neurodesk, Jetstream, and local Ollama model profiles.
set_opencode_model_profile() {
    local profile="$1"

    ensure_opencode_config_file
    if [ ! -f "${OPENCODE_CONFIG_FILE}" ]; then
        return 0
    fi

    if ! command -v python3 >/dev/null 2>&1; then
        echo "Warning: python3 not found; cannot update OpenCode model profile." >&2
        return 1
    fi

    python3 - "${OPENCODE_CONFIG_FILE}" "${OPENCODE_DEFAULT_CONFIG_FILE}" "${profile}" <<'PY'
import json
import os
import sys

config_path, default_path, profile = sys.argv[1:4]

with open(config_path, "r", encoding="utf-8") as f:
    cfg = json.load(f)

default_cfg = {}
if os.path.exists(default_path):
    with open(default_path, "r", encoding="utf-8") as f:
        default_cfg = json.load(f)

providers = cfg.get("provider", {})
default_providers = default_cfg.get("provider", {})

ollama_host = os.environ.get("OLLAMA_HOST", "http://127.0.0.1:11434").rstrip("/")
if ollama_host.endswith("/v1"):
    ollama_host = ollama_host[:-3]

profile_defaults = {
    "neurodesk": {
        "provider_key": "neurodesk",
        "model": "neurodesk/devstral-small-2",
    },
    "jetstream": {
        "provider_key": "jetstream",
        "model": "jetstream/gpt-oss-120b",
    },
    "ollama": {
        "provider_key": "ollama",
        "model": "ollama/qwen3-coder-next:latest",
        "provider_fallback": {
            "npm": "@ai-sdk/openai-compatible",
            "name": "Local Ollama",
            "options": {
                "baseURL": f"{ollama_host}/v1",
            },
            "models": {
                "neurodesk": {"name": "neurodesk"},
                "qwen3-coder-next:latest": {"name": "qwen3-coder-next:latest"},
                "devstral:latest": {"name": "devstral:latest"},
                "codellama:7b-code": {"name": "codellama:7b-code"},
            },
        },
    },
}

if profile not in profile_defaults:
    raise SystemExit(f"Unknown profile: {profile}")

def ensure_provider(provider_key, provider_fallback=None):
    if provider_key in providers:
        return
    if provider_key in default_providers:
        providers[provider_key] = default_providers[provider_key]
    elif provider_fallback is not None:
        providers[provider_key] = provider_fallback

# Keep all known providers visible in OpenCode, even when one profile is selected.
ensure_provider("neurodesk")
ensure_provider("jetstream")
ensure_provider("jetstream-deepseek")
ensure_provider("ollama", profile_defaults["ollama"]["provider_fallback"])

profile_cfg = profile_defaults[profile]

# Keep Ollama target in sync when users override OLLAMA_HOST.
if isinstance(providers.get("ollama"), dict):
    options = providers["ollama"].setdefault("options", {})
    options["baseURL"] = f"{ollama_host}/v1"
    models = providers["ollama"].setdefault("models", {})
    for model_name in (
        "neurodesk",
        "qwen3-coder-next:latest",
        "devstral:latest",
        "codellama:7b-code",
    ):
        models.setdefault(model_name, {"name": model_name})

cfg["provider"] = providers
cfg["model"] = profile_cfg["model"]

tmp_path = f"{config_path}.tmp"
with open(tmp_path, "w", encoding="utf-8") as f:
    json.dump(cfg, f, indent=2)
    f.write("\n")
os.replace(tmp_path, config_path)
PY
}

# Check whether a local Ollama server appears reachable.
is_local_ollama_available() {
    local ollama_host="${OLLAMA_HOST:-http://127.0.0.1:11434}"
    ollama_host="${ollama_host%/}"
    ollama_host="${ollama_host%/v1}"

    if ! command -v curl >/dev/null 2>&1; then
        return 1
    fi

    curl -fsS "${ollama_host}/api/tags" >/dev/null 2>&1
}

# Ensure OpenCode has a usable model profile.
ensure_opencode_model_profile() {
    local bashrc_file="${HOME}/.bashrc"
    local key_from_bashrc
    local entered_key
    local tmp_bashrc
    local choice
    local requested_profile="${OPENCODE_MODEL_PROFILE:-}"

    if [ -n "${requested_profile}" ]; then
        case "${requested_profile}" in
            neurodesk|jetstream|ollama)
                if [ "${requested_profile}" = "ollama" ] && ! is_local_ollama_available; then
                    echo "OPENCODE_MODEL_PROFILE=ollama requested, but local Ollama is not reachable at ${OLLAMA_HOST:-http://127.0.0.1:11434} yet." >&2
                fi
                set_opencode_model_profile "${requested_profile}"
                return 0
                ;;
            *)
                echo "Invalid OPENCODE_MODEL_PROFILE='${requested_profile}'. Expected: neurodesk, jetstream, or ollama." >&2
                ;;
        esac
    fi

    # Prefer local Ollama in explicit local-LLM sessions.
    if [ "${START_LOCAL_LLMS:-0}" = "1" ]; then
        if ! is_local_ollama_available; then
            echo "START_LOCAL_LLMS=1 is set, but local Ollama is not reachable at ${OLLAMA_HOST:-http://127.0.0.1:11434} yet." >&2
        fi
        set_opencode_model_profile ollama
        return 0
    fi

    # Use already-exported key when available.
    if [ -n "${NEURODESK_API_KEY:-}" ]; then
        set_opencode_model_profile neurodesk
        return 0
    fi

    # Try to recover key from an existing .bashrc export line first.
    if [ -f "${bashrc_file}" ]; then
        key_from_bashrc=$(sed -nE \
            -e "s/^[[:space:]]*export[[:space:]]+NEURODESK_API_KEY='([^']+)'[[:space:]]*$/\1/p" \
            -e 's/^[[:space:]]*export[[:space:]]+NEURODESK_API_KEY="([^"]+)"[[:space:]]*$/\1/p' \
            -e 's/^[[:space:]]*export[[:space:]]+NEURODESK_API_KEY=([^[:space:]#]+)[[:space:]]*$/\1/p' \
            "${bashrc_file}" | tail -n 1)
        if [ -n "${key_from_bashrc}" ]; then
            export NEURODESK_API_KEY="${key_from_bashrc}"
            set_opencode_model_profile neurodesk
            return 0
        fi
    fi

    if [ ! -t 0 ]; then
        if is_local_ollama_available; then
            echo "NEURODESK_API_KEY is not set. Local Ollama is available; using local model." >&2
            set_opencode_model_profile ollama
        else
            echo "NEURODESK_API_KEY is not set. Using Jetstream model for this session." >&2
            set_opencode_model_profile jetstream
        fi
        return 0
    fi

    echo "NEURODESK_API_KEY is not set."
    echo "1) Create key at https://llm.neurodesk.org and use Neurodesk model"
    echo "2) Skip key creation and use Jetstream model"
    echo "3) Use local Ollama model (requires running Ollama server)"

    while true; do
        read -r -p "Choose [1/2/3]: " choice
        case "${choice}" in
            1)
                echo "Go to https://llm.neurodesk.org, create an API key, then paste it below."
                while [ -z "${entered_key:-}" ]; do
                    read -r -s -p "Paste your API key: " entered_key
                    echo
                    if [ -z "${entered_key}" ]; then
                        echo "API key cannot be empty. Please try again."
                    fi
                done

                # Persist the key to ~/.bashrc and export it for this process.
                touch "${bashrc_file}"
                tmp_bashrc=$(mktemp)
                grep -v '^[[:space:]]*export[[:space:]]\+NEURODESK_API_KEY=' "${bashrc_file}" > "${tmp_bashrc}" || true
                echo "" >> "${tmp_bashrc}"
                echo "# Neurodesk API key for OpenCode" >> "${tmp_bashrc}"
                printf "export NEURODESK_API_KEY='%s'\n" "${entered_key}" >> "${tmp_bashrc}"
                mv "${tmp_bashrc}" "${bashrc_file}"

                export NEURODESK_API_KEY="${entered_key}"
                set_opencode_model_profile neurodesk
                echo "Saved NEURODESK_API_KEY to ${bashrc_file}."
                return 0
                ;;
            2)
                echo "Skipping key creation. Switching OpenCode to Jetstream model."
                set_opencode_model_profile jetstream
                return 0
                ;;
            3)
                if ! is_local_ollama_available; then
                    echo "Local Ollama server not reachable at ${OLLAMA_HOST:-http://127.0.0.1:11434} right now."
                    echo "Switching anyway; start Ollama before sending requests."
                fi
                echo "Switching OpenCode to local Ollama model."
                set_opencode_model_profile ollama
                return 0
                ;;
            *)
                echo "Invalid choice. Enter 1, 2, or 3."
                ;;
        esac
    done
}

ensure_opencode_model_profile

# Start OpenCode using the actual binary path (not this wrapper)
export TERM=xterm-direct
exec /usr/bin/opencode "$@"
